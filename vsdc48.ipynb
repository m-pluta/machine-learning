{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Image handling\n",
    "from PIL import Image\n",
    "\n",
    "# Numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducability\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we establish the directory to where our data is stored, and where we ultimately want to store our cleaned, preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'chinese-handwriting-recognition-hsk-1/chinese-handwriting/'\n",
    "\n",
    "TRAIN_DIR = os.path.join(DIR, 'CASIA-HWDB_Train/Train/')\n",
    "TEST_DIR = os.path.join(DIR, 'CASIA-HWDB_Test/Test/')\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "DATA_TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "DATA_VAL_DIR = os.path.join(DATA_DIR, 'val')\n",
    "DATA_TEST_DIR = os.path.join(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets have a look at how many data classes we have in the dataset. The dataset has been split into test and train already so lets check how many classes we have in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classes_train = os.listdir(TRAIN_DIR)\n",
    "image_classes_test = os.listdir(TEST_DIR)\n",
    "print(len(image_classes_train), image_classes_train)\n",
    "print(len(image_classes_test), image_classes_test)\n",
    "\n",
    "if image_classes_train == image_classes_test:\n",
    "    print('The same classes are in each folder')\n",
    "    # image_classes = ['零', '一', '二']\n",
    "    # image_classes = ['零', '一', '二', '三', '四', '五', '六', '七', '八', '九', '十']\n",
    "    image_classes = image_classes_train[:20]\n",
    "    # image_classes = image_classes_train\n",
    "else:\n",
    "    print('The two folders contain different classes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets combine the the train and test data into one directory for simplicity.\n",
    "\n",
    "We start by creating a new directory for our preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old data\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "    \n",
    "#Create the new directory\n",
    "os.mkdir(DATA_DIR)\n",
    "for dir in (DATA_TRAIN_DIR, DATA_TEST_DIR, DATA_VAL_DIR):\n",
    "    for image_class in image_classes:\n",
    "        path = os.path.join(dir, image_class)\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the train and test data into one directory. This is specified by the `DATA_DIR` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store how many of each class there are\n",
    "class_counts = {}\n",
    "max_images_per_class = 200\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT   = 0.15\n",
    "TEST_SPLIT  = 0.15\n",
    "\n",
    "for image_class in image_classes:\n",
    "    images = []\n",
    "\n",
    "    # Fetch images from train dir\n",
    "    train_path = os.path.join(TRAIN_DIR, image_class)\n",
    "    images += [os.path.join(train_path, file) for file in os.listdir(train_path)]\n",
    "\n",
    "    # Fetch images from test dir\n",
    "    test_path = os.path.join(TEST_DIR, image_class)\n",
    "    images += [os.path.join(test_path, file) for file in os.listdir(test_path)]\n",
    "\n",
    "    # Update class_counts\n",
    "    num_images = min(len(images), max_images_per_class)\n",
    "    class_counts[image_class] = num_images\n",
    "\n",
    "    # Select the images\n",
    "    # images = images[:num_images]\n",
    "    images = random.choices(images, k=num_images)\n",
    "\n",
    "    # Define the bounds of the splits\n",
    "    train = (DATA_TRAIN_DIR, 0, int(TRAIN_SPLIT*num_images))\n",
    "    test = (DATA_TEST_DIR, train[2], train[2] + int(TEST_SPLIT*num_images))\n",
    "    val = (DATA_VAL_DIR, test[2], num_images)\n",
    "\n",
    "    # Iterate over the splits and images and copy them to the data directory\n",
    "    for split in (train, test, val):\n",
    "        for i, image in enumerate(images[split[1]:split[2]]):\n",
    "            new_filename = f\"{i+1}.png\"\n",
    "            destination_path = os.path.join(split[0], image_class, new_filename)\n",
    "            shutil.copy(image, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check how many images we have in each class and see how balanced the classes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_counts = pd.DataFrame.from_dict(class_counts, orient='index', columns=['Count'])\n",
    "df_class_counts.reset_index(inplace=True)\n",
    "df_class_counts.columns = ['Character', 'Count']\n",
    "\n",
    "print(df_class_counts)\n",
    "print(df_class_counts.describe([0.05, 0.25, 0.75, 0.95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the classes are fairly balanced, let's quickly have a look at the slight imbalance in the classes.\n",
    "We can arbitrarily choose a balance metric such as outside the range of 2 standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_class_counts['Count'].mean()\n",
    "std = df_class_counts['Count'].std()\n",
    "threshold = 2 * std\n",
    "\n",
    "outlier_counts = df_class_counts[np.abs(df_class_counts['Count'] - mean) > threshold]\n",
    "print(outlier_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the imbalance is very minimal. For now we can move on to the next step, and revisit this if the performance of our model is being affected by this imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets iterate through all the images and and confirm they are all `.png` and black & white "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each subdirectory and file in the directory\n",
    "for subdir, dirs, images in os.walk(DATA_DIR):\n",
    "    for image in images:\n",
    "        if image.lower().endswith('.png'):\n",
    "            # Construct the full path\n",
    "            file_path = os.path.join(subdir, image)\n",
    "\n",
    "            # Open the image\n",
    "            with Image.open(file_path) as img:\n",
    "                # Check if the image is not grayscale\n",
    "                if img.mode != 'L':\n",
    "                    print(f\"{file_path} is not a black and white image.\")\n",
    "        else:\n",
    "            print(\"Not a png file: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's iterate through all the images and check if any have an aspect ratio that is not 1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = []\n",
    "\n",
    "# Iterate through all the images and check if any do not have a 1:1 aspect ratio\n",
    "for subdir, dirs, images in os.walk(DATA_DIR):\n",
    "    for image in images:\n",
    "        file_path = os.path.join(subdir, image)\n",
    "        with Image.open(file_path) as img:\n",
    "            image_sizes.append(img.size[0])\n",
    "            \n",
    "            if img.size[0] != img.size[1]:\n",
    "                print(file_path, img.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the images are square, this makes it easier to investigate their image sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Smallest dimension: {min(image_sizes)}\")\n",
    "\n",
    "# Plotting\n",
    "sns.displot(image_sizes)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the graph, the images are not all the same size. This will cause issues when we try to train the model, so we need to resize all the images to the same size. Additionally one of the images is only 3x3 which is way too small to be useful, so we will remove that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the images and resize to a fixed size\n",
    "for subdir, dirs, images in os.walk(DATA_DIR):\n",
    "    for image in images:\n",
    "        file_path = os.path.join(subdir, image)\n",
    "        with Image.open(file_path) as img:\n",
    "            current_size = img.size\n",
    "            if img.size[0] < 5:\n",
    "                os.remove(file_path)\n",
    "            \n",
    "            IMAGE_SIZE = 50\n",
    "            \n",
    "            resized_img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            resized_img.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the images are the same size, we can begin with augmenting the images. This will involve randomly flipping rotating the images to create more data for the model to train on and prevent overfitting.\n",
    "Note, there are many more augmentation techniques that can be used such as scaling, randomly cropping, randomly erasing, distorting, and blurring, but for now we will only use rotation and flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many augmented images should be generated as a percentage of the current\n",
    "percent_augmented = 0.5 # 0 to 1\n",
    "augments = [(0, 90), (0, 180), (0, 270), (1, 0), (1, 90), (1, 180), (1, 270)]\n",
    "\n",
    "# Iterate through all the images and resize to a fixed size\n",
    "for subdir, dirs, images in os.walk(DATA_TRAIN_DIR):\n",
    "    for image in images:\n",
    "        file_path = os.path.join(subdir, image)\n",
    "        with Image.open(file_path) as img:\n",
    "            if random.random() <= percent_augmented:\n",
    "                augmented_img = img\n",
    "\n",
    "                augment = random.choice(augments)\n",
    "\n",
    "                if augment[0]:\n",
    "                    augmented_img = augmented_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                if augment[1]:\n",
    "                    augmented_img = augmented_img.rotate(augment[1])\n",
    "\n",
    "                name, extension = image.rsplit('.', 1)\n",
    "                save_file_path = os.path.join(subdir, f\"{name}.aug.{extension}\")\n",
    "\n",
    "                augmented_img.save(save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "png_files = glob.glob(f\"{DATA_DIR}/**/*.png\", recursive=True)\n",
    "aug_png_files = glob.glob(f\"{DATA_DIR}/**/*.aug.png\", recursive=True)\n",
    "\n",
    "print(f\"Number of .png files in the data directory: {len(png_files)}\")\n",
    "print(f\"Number of .aug.png files in the data directory: {len(aug_png_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean and standard deviation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the images and resize to a fixed size\n",
    "pixel_sum = np.zeros(1)\n",
    "pixel_sum_sq = np.zeros(1)\n",
    "pixel_count = 0\n",
    "pixels_per_image = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "for subdir, dirs, images in os.walk(DATA_DIR):\n",
    "    for image in images:\n",
    "        file_path = os.path.join(subdir, image)\n",
    "        with Image.open(file_path) as img:\n",
    "            image_array = np.array(img) / 255.0\n",
    "            pixel_sum += image_array.sum()\n",
    "            pixel_sum_sq += (image_array ** 2).sum()\n",
    "            pixel_count += pixels_per_image\n",
    "\n",
    "mean = pixel_sum / pixel_count\n",
    "std = np.sqrt(pixel_sum_sq / pixel_count - mean ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the CNN model we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet\n",
    "resnet = resnet50(pretrained=True)\n",
    "\n",
    "# Remove the last fully connected layer for feature extraction\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define where the images are stored, and appropriate loaders for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = ImageFolder(root=DATA_TRAIN_DIR, transform=transform)\n",
    "val_folder = ImageFolder(root=DATA_VAL_DIR, transform=transform)\n",
    "test_folder = ImageFolder(root=DATA_TEST_DIR, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_folder, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_folder, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_folder, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function, where given a loader, it will extract features from the images using the CNN model we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(folder, loader, model):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, label in loader:\n",
    "            images = images.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            output = model(images)\n",
    "            \n",
    "            feature_batch = output.cpu().detach().numpy().flatten().reshape(-1, 2048)\n",
    "            features.extend(feature_batch)\n",
    "            labels.extend(label.numpy())\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_folder, train_loader, resnet)\n",
    "val_features, val_labels = extract_features(test_folder, val_loader, resnet)\n",
    "test_features, test_labels = extract_features(val_folder, test_loader, resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert labels to class names\n",
    "def labels_to_classes(labels, folder: ImageFolder):\n",
    "    # Invert the mapping\n",
    "    idx_to_class = {v: k for k, v in folder.class_to_idx.items()}\n",
    "    \n",
    "    # Map all index labels to the corresponding class names\n",
    "    return [idx_to_class[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16, solver='newton-cg')\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(train_features, labels_to_classes(train_labels, train_folder))\n",
    "\n",
    "val_pred = logreg.predict(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(labels_to_classes(val_labels, val_folder), val_pred, labels=list(image_classes))\n",
    "list(image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=800)\n",
    "\n",
    "fprop = fm.FontProperties(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc')\n",
    "\n",
    "# create heatmap\n",
    "ax = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=sns.color_palette(\"Blues\", as_cmap=True) ,fmt='g')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "tick_marks = np.arange(len(image_classes)) + 0.5\n",
    "plt.xticks(tick_marks, image_classes, fontproperties=fprop)\n",
    "plt.yticks(tick_marks, image_classes, rotation=0, fontproperties=fprop)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels_to_classes(val_labels, val_folder), val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
